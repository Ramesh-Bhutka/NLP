{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ancient-texture",
   "metadata": {},
   "source": [
    "## Practical 7\n",
    "- A. Define grammer using nltk. Analyze a sentence using the same.\n",
    "- B. Accept the input string with Regular expression of FA: 101+\n",
    "- C. Accept the input string with Regular expression of FA: (a+b)*bba\n",
    "- D. Implementation of Deductive Chart Parsing using context free grammar and a given sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-cross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:- RAMESH BHUTKA\n",
      "SAP ID:- 5300419003\n"
     ]
    }
   ],
   "source": [
    "print(\"NAME:- RAMESH BHUTKA\")\n",
    "print(\"SAP ID:- 5300419003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-tradition",
   "metadata": {},
   "source": [
    "### A. Define grammer using nltk. Analyze a sentence using the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "interior-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/ch08.html#ex-elephant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "challenging-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-d67a3aacaf1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m grammar1 = nltk.CFG.fromstring(\"\"\"\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mS\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNP\u001b[0m \u001b[0mVP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mVP\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mV\u001b[0m \u001b[0mNP\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mV\u001b[0m \u001b[0mNP\u001b[0m \u001b[0mPP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mPP\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mP\u001b[0m \u001b[0mNP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "placed-station",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'check_coverage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-41e8e5a16716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Mary saw Bob\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m      \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\nltk\\parse\\recursivedescent.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'check_coverage'"
     ]
    }
   ],
   "source": [
    "sent = \"Mary saw Bob\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(sent):\n",
    "     print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-postcard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "crucial-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import *\n",
    "grammar = r\"\"\"\n",
    "NP: {<DT|JJ|NN.*>+} # Chunk sequences of DT, JJ, NN\n",
    "PP: {<IN><NP>} # Chunk prepositions followed by NP\n",
    "VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "CLAUSE: {<NP><VP>} # Chunk NP, VP\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "(\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "respective-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
    "(\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
    "(\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "returning-syndication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n"
     ]
    }
   ],
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "print(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unknown-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP the rabbit)\n"
     ]
    }
   ],
   "source": [
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smart-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP chased (NP the rabbit)))\n"
     ]
    }
   ],
   "source": [
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "print(tree4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-vocabulary",
   "metadata": {},
   "source": [
    "### B. Accept the input string with Regular expression of FA: 101+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "valid-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    ->  Rejected\n",
      "10101    ->  Rejected\n",
      "101    ->  Accepted\n",
      "10111    ->  Accepted\n",
      "01010    ->  Rejected\n",
      "    ->  Rejected\n"
     ]
    }
   ],
   "source": [
    "def FA(s):\n",
    "#if the length is less than 3 then it can't be accepted, Therefore end the process.\n",
    "    if len(s)<3:\n",
    "        return \"Rejected\"\n",
    "#first three characters are fixed. Therefore checking them using index\n",
    "    if s[0]=='1':\n",
    "        if s[1]=='0':\n",
    "            if s[2]=='1':\n",
    "                # After index 2 only \"1\" can appear. Therefore break the process if any other character is detected\n",
    "                for i in range(3,len(s)):\n",
    "                    if s[i]!='1':\n",
    "                        return \"Rejected\"\n",
    "                return \"Accepted\"\n",
    "            return \"Rejected\"\n",
    "        return \"Rejected\"\n",
    "    return \"Rejected\"\n",
    "inputs=['1','10101','101','10111','01010',\"\"]\n",
    "for i in inputs:\n",
    "    print(i,\"   -> \",FA(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-champagne",
   "metadata": {},
   "source": [
    "### C. Accept the input string with Regular expression of FA: (a+b)*bba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "atlantic-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bba    ->  Accepted\n",
      "ababbba    ->  Accepted\n",
      "abba    ->  Accepted\n",
      "abb    ->  Rejected\n",
      "baba    ->  Rejected\n",
      "bbb    ->  Rejected\n",
      "    ->  Rejected\n"
     ]
    }
   ],
   "source": [
    "def FA(s):\n",
    "    size=0\n",
    "#scan complete string and make sure that it contains only 'a' & 'b'\n",
    "    for i in s:\n",
    "        if i=='a' or i=='b':\n",
    "            size+=1\n",
    "        else:\n",
    "            return \"Rejected\"\n",
    "#After checking that it contains only 'a' & 'b'\n",
    "#check it's length it should be 3 atleast\n",
    "    if size>=3:\n",
    "#check the last 3 elements\n",
    "        if s[size-3]=='b':\n",
    "            if s[size-2]=='b':    \n",
    "                if s[size-1]=='a':\n",
    "                    return \"Accepted\"\n",
    "                return \"Rejected\"\n",
    "            return \"Rejected\"\n",
    "        return \"Rejected\"\n",
    "    return \"Rejected\"\n",
    "\n",
    "inputs=['bba', 'ababbba', 'abba','abb', 'baba','bbb','']\n",
    "for i in inputs:\n",
    "    print(i,\"   -> \",FA(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-bachelor",
   "metadata": {},
   "source": [
    "### D. Implementation of Deductive Chart Parsing using context free grammar and a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flying-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n"
     ]
    }
   ],
   "source": [
    "from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "tokens = \"Jack saw Bob with my cookie\".split()\n",
    "grammar = toy_pcfg2\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-buddy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
