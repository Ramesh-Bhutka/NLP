{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deluxe-poetry",
   "metadata": {},
   "source": [
    "## Practical 6\n",
    "### Illustrate part of speech tagging\n",
    "- A. Part of speech Tagging and chunking of user defined text\n",
    "- B. Named Entity recognition of user defined text.\n",
    "- C. Named Entity recognition with diagram using NLTK corpus – treebank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "speaking-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:- RAMESH BHUTKA\n",
      "SAP ID:- 5300419003\n"
     ]
    }
   ],
   "source": [
    "print(\"NAME:- RAMESH BHUTKA\")\n",
    "print(\"SAP ID:- 5300419003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-beads",
   "metadata": {},
   "source": [
    "### A. Part of speech Tagging and chunking of user defined text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "existing-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accessible-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After split:  ['NLP', 'is', 'very', 'interesting', 'subject']\n",
      "After token :  [('NLP', 'NNP'), ('is', 'VBZ'), ('very', 'RB'), ('interesting', 'JJ'), ('subject', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBZ>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk NLP/NNP is/VBZ)\n",
      "  very/RB\n",
      "  (mychunk interesting/JJ)\n",
      "  (mychunk subject/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text='NLP is very interesting subject'.split()\n",
    "print(\"After split: \",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After token : \",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBZ>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "banner-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After split:  ['NLP', 'is', 'very', 'interesting', 'subject']\n",
      "After token :  [('NLP', 'NNP'), ('is', 'VBZ'), ('very', 'RB'), ('interesting', 'JJ'), ('subject', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk NLP/NNP)\n",
      "  is/VBZ\n",
      "  very/RB\n",
      "  (mychunk interesting/JJ)\n",
      "  (mychunk subject/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text='NLP is very interesting subject'.split()\n",
    "print(\"After split: \",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After token : \",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "educated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After split:  ['We', 'saw', 'the', 'yellow', 'dog']\n",
      "After token :  [('We', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('yellow', 'JJ'), ('dog', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  We/PRP\n",
      "  (mychunk saw/VBD)\n",
      "  the/DT\n",
      "  (mychunk yellow/JJ)\n",
      "  (mychunk dog/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text='We saw the yellow dog'.split()\n",
    "print(\"After split: \",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After token : \",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-writing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-match",
   "metadata": {},
   "source": [
    "### B. Named Entity recognition of user defined text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "isolated-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "absolute-aaron",
   "metadata": {},
   "source": [
    "### C. Named Entity recognition with diagram using NLTK corpus – treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "younger-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tender-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', 'wsj_0005.mrg', 'wsj_0006.mrg', 'wsj_0007.mrg', 'wsj_0008.mrg', 'wsj_0009.mrg', 'wsj_0010.mrg', 'wsj_0011.mrg', 'wsj_0012.mrg', 'wsj_0013.mrg', 'wsj_0014.mrg', 'wsj_0015.mrg', 'wsj_0016.mrg', 'wsj_0017.mrg', 'wsj_0018.mrg', 'wsj_0019.mrg', 'wsj_0020.mrg', 'wsj_0021.mrg', 'wsj_0022.mrg', 'wsj_0023.mrg', 'wsj_0024.mrg', 'wsj_0025.mrg', 'wsj_0026.mrg', 'wsj_0027.mrg', 'wsj_0028.mrg', 'wsj_0029.mrg', 'wsj_0030.mrg', 'wsj_0031.mrg', 'wsj_0032.mrg', 'wsj_0033.mrg', 'wsj_0034.mrg', 'wsj_0035.mrg', 'wsj_0036.mrg', 'wsj_0037.mrg', 'wsj_0038.mrg', 'wsj_0039.mrg', 'wsj_0040.mrg', 'wsj_0041.mrg', 'wsj_0042.mrg', 'wsj_0043.mrg', 'wsj_0044.mrg', 'wsj_0045.mrg', 'wsj_0046.mrg', 'wsj_0047.mrg', 'wsj_0048.mrg', 'wsj_0049.mrg', 'wsj_0050.mrg', 'wsj_0051.mrg', 'wsj_0052.mrg', 'wsj_0053.mrg', 'wsj_0054.mrg', 'wsj_0055.mrg', 'wsj_0056.mrg', 'wsj_0057.mrg', 'wsj_0058.mrg', 'wsj_0059.mrg', 'wsj_0060.mrg', 'wsj_0061.mrg', 'wsj_0062.mrg', 'wsj_0063.mrg', 'wsj_0064.mrg', 'wsj_0065.mrg', 'wsj_0066.mrg', 'wsj_0067.mrg', 'wsj_0068.mrg', 'wsj_0069.mrg', 'wsj_0070.mrg', 'wsj_0071.mrg', 'wsj_0072.mrg', 'wsj_0073.mrg', 'wsj_0074.mrg', 'wsj_0075.mrg', 'wsj_0076.mrg', 'wsj_0077.mrg', 'wsj_0078.mrg', 'wsj_0079.mrg', 'wsj_0080.mrg', 'wsj_0081.mrg', 'wsj_0082.mrg', 'wsj_0083.mrg', 'wsj_0084.mrg', 'wsj_0085.mrg', 'wsj_0086.mrg', 'wsj_0087.mrg', 'wsj_0088.mrg', 'wsj_0089.mrg', 'wsj_0090.mrg', 'wsj_0091.mrg', 'wsj_0092.mrg', 'wsj_0093.mrg', 'wsj_0094.mrg', 'wsj_0095.mrg', 'wsj_0096.mrg', 'wsj_0097.mrg', 'wsj_0098.mrg', 'wsj_0099.mrg', 'wsj_0100.mrg', 'wsj_0101.mrg', 'wsj_0102.mrg', 'wsj_0103.mrg', 'wsj_0104.mrg', 'wsj_0105.mrg', 'wsj_0106.mrg', 'wsj_0107.mrg', 'wsj_0108.mrg', 'wsj_0109.mrg', 'wsj_0110.mrg', 'wsj_0111.mrg', 'wsj_0112.mrg', 'wsj_0113.mrg', 'wsj_0114.mrg', 'wsj_0115.mrg', 'wsj_0116.mrg', 'wsj_0117.mrg', 'wsj_0118.mrg', 'wsj_0119.mrg', 'wsj_0120.mrg', 'wsj_0121.mrg', 'wsj_0122.mrg', 'wsj_0123.mrg', 'wsj_0124.mrg', 'wsj_0125.mrg', 'wsj_0126.mrg', 'wsj_0127.mrg', 'wsj_0128.mrg', 'wsj_0129.mrg', 'wsj_0130.mrg', 'wsj_0131.mrg', 'wsj_0132.mrg', 'wsj_0133.mrg', 'wsj_0134.mrg', 'wsj_0135.mrg', 'wsj_0136.mrg', 'wsj_0137.mrg', 'wsj_0138.mrg', 'wsj_0139.mrg', 'wsj_0140.mrg', 'wsj_0141.mrg', 'wsj_0142.mrg', 'wsj_0143.mrg', 'wsj_0144.mrg', 'wsj_0145.mrg', 'wsj_0146.mrg', 'wsj_0147.mrg', 'wsj_0148.mrg', 'wsj_0149.mrg', 'wsj_0150.mrg', 'wsj_0151.mrg', 'wsj_0152.mrg', 'wsj_0153.mrg', 'wsj_0154.mrg', 'wsj_0155.mrg', 'wsj_0156.mrg', 'wsj_0157.mrg', 'wsj_0158.mrg', 'wsj_0159.mrg', 'wsj_0160.mrg', 'wsj_0161.mrg', 'wsj_0162.mrg', 'wsj_0163.mrg', 'wsj_0164.mrg', 'wsj_0165.mrg', 'wsj_0166.mrg', 'wsj_0167.mrg', 'wsj_0168.mrg', 'wsj_0169.mrg', 'wsj_0170.mrg', 'wsj_0171.mrg', 'wsj_0172.mrg', 'wsj_0173.mrg', 'wsj_0174.mrg', 'wsj_0175.mrg', 'wsj_0176.mrg', 'wsj_0177.mrg', 'wsj_0178.mrg', 'wsj_0179.mrg', 'wsj_0180.mrg', 'wsj_0181.mrg', 'wsj_0182.mrg', 'wsj_0183.mrg', 'wsj_0184.mrg', 'wsj_0185.mrg', 'wsj_0186.mrg', 'wsj_0187.mrg', 'wsj_0188.mrg', 'wsj_0189.mrg', 'wsj_0190.mrg', 'wsj_0191.mrg', 'wsj_0192.mrg', 'wsj_0193.mrg', 'wsj_0194.mrg', 'wsj_0195.mrg', 'wsj_0196.mrg', 'wsj_0197.mrg', 'wsj_0198.mrg', 'wsj_0199.mrg']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "print(treebank.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "minimal-surgeon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP love/VBP (NP open/JJ source/NN))\n"
     ]
    }
   ],
   "source": [
    "text = 'I love open source'\n",
    "# tokenize to words\n",
    "words = nltk.tokenize.word_tokenize(text)\n",
    "# POS tag the words\n",
    "words_tagged = nltk.pos_tag(words)\n",
    "grammer = 'NP: {<JJ><NN>}'\n",
    "parser = nltk.RegexpParser(grammer)\n",
    "tree = parser.parse(words_tagged)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "disciplinary-conflict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP)\n",
      "  T./NNP\n",
      "  Mossman/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE University/NNP)\n",
      "  of/IN\n",
      "  (NE Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (NE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(nltk.ne_chunk(sent, binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "third-healing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION University/NNP)\n",
      "  of/IN\n",
      "  (PERSON Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (GPE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(sent)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-enough",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
